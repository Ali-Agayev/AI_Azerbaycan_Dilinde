from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List
import uvicorn
import os
from pathlib import Path

# Torch opsionaldir â€” Railway-dÉ™ lazim deyil (GPU Kaggle-dadÄ±r)
try:
    import torch
    TORCH_VAR = True
except ImportError:
    TORCH_VAR = False

# Model vÉ™ tokenizer opsionaldir â€” yalnÄ±z lokal dev-dÉ™ iÅŸlÉ™yir
try:
    from model import IsmayilModeli
    from tokenizer import CharTokenizator
    MODEL_VAR = True
except ImportError:
    MODEL_VAR = False

from kaggle_client import is_gondÉ™r, is_veziyyeti, is_siyahisi

# FastAPI tÉ™tbiqini yaradÄ±rÄ±q
ismayil_server = FastAPI(title="Ä°smayÄ±lÄ±n ÅÉ™xsi AI Serveri")

# CORS tÉ™nzimlÉ™mÉ™lÉ™ri
# BÃ¼tÃ¼n mÉ™nbÉ™lÉ™rÉ™ icazÉ™ veririk ki, Vercel rahat qoÅŸulsun
ismayil_server.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Qlobal dÉ™yiÅŸÉ™nlÉ™r â€” Model vÉ™ Tokenizatoru burada saxlayÄ±rÄ±q
cihaz = ('cuda' if (TORCH_VAR and torch.cuda.is_available()) else 'cpu') if TORCH_VAR else 'cpu'
tokenizator = None
ismayil_modeli = None

def ai_ni_bashlat():
    """
    Modeli vÉ™ Tokenizatoru yaddaÅŸdan yÃ¼klÉ™yÉ™n funksiya.
    Torch vÉ™ ya model faylÄ± yoxdursa False qaytarÄ±r (Railway-dÉ™ normal haldir).
    """  
    if not TORCH_VAR or not MODEL_VAR:
        return False  # Railway-dÉ™ torch/model yoxdur â€” chat endpoint disabled
    global tokenizator, ismayil_modeli
    if ismayil_modeli is None:
        # FayllarÄ±n varlÄ±ÄŸÄ±nÄ± yoxlayÄ±rÄ±q
        if not os.path.exists('tokenizer.json') or not os.path.exists('ismayil_model.pth'):
            return False
            
        # Tokenizatoru yÃ¼klÉ™yirik
        tokenizator = CharTokenizator.yukle('tokenizer.json')
        # Model memarlÄ±ÄŸÄ±nÄ± yaradÄ±rÄ±q
        ismayil_modeli = IsmayilModeli(tokenizator.luget_olcusu)
        # Ã–yrÉ™nilmiÅŸ Ã§É™kilÉ™ri (weights) modelÉ™ yÃ¼klÉ™yirik
        ismayil_modeli.load_state_dict(torch.load('ismayil_model.pth', map_location=cihaz))
        ismayil_modeli.to(cihaz)
        ismayil_modeli.eval() # Modeli yalnÄ±z cavab vermÉ™ (inference) rejiminÉ™ salÄ±rÄ±q
        print("Ä°smayÄ±l AI uÄŸurla iÅŸÉ™ dÃ¼ÅŸdÃ¼ vÉ™ suallarÄ±nÄ±zÄ± gÃ¶zlÉ™yir!")
    return True

# API istÉ™klÉ™ri Ã¼Ã§Ã¼n mÉ™lumat strukturlarÄ± (Pydantic modellÉ™ri)
class Mesaj(BaseModel):
    role: str # 'user' vÉ™ ya 'assistant'
    content: str # MesajÄ±n mÉ™tni

class ChatIsteyi(BaseModel):
    messages: List[Mesaj]

@ismayil_server.get("/")
async def ana_sehife():
    # Serverin aktiv olub-olmadÄ±ÄŸÄ±nÄ± yoxlamaq Ã¼Ã§Ã¼n kiÃ§ik endpoint
    return {"status": "online", "model": "Ä°smayÄ±l Custom Transformer"}

@ismayil_server.post("/v1/chat/completions")
async def chat_cavabi(istek: ChatIsteyi):
    """
    Bu É™sas hissÉ™dir. Ä°stifadÉ™Ã§idÉ™n gÉ™lÉ™n mesajÄ± AI-yÉ™ gÃ¶ndÉ™rir vÉ™ cavab alÄ±r.
    """
    # AI-nin hazÄ±r olub-olmadÄ±ÄŸÄ±nÄ± yoxlayÄ±rÄ±q
    if not ai_ni_bashlat():
        # Railway-dÉ™ lokal model yoxdursa "baÄŸÄ±ÅŸlayÄ±n" mock cavabÄ± qaytarÄ±rÄ±q (xÉ™ta vermÉ™kdÉ™n yaxÅŸÄ±dÄ±r)
        return {
            "choices": [
                {
                    "message": {
                        "role": "assistant",
                        "content": "Salam! MÉ™n hazÄ±rda 'Video DÃ¼zÉ™lt' rejimindÉ™, yÃ¼ngÃ¼l (Railway) serverdÉ™ iÅŸlÉ™yirÉ™m. Ã–z 'Custom Transformer' beynim (PyTorch) bu serverÉ™ yÃ¼klÉ™nmÉ™yib. MÉ™nlÉ™ real sÃ¶hbÉ™t etmÉ™k Ã¼Ã§Ã¼n mÉ™ni Ã¶z kompÃ¼terinizdÉ™ (Anaconda ilÉ™) Ã§alÄ±ÅŸdÄ±rÄ±n vÉ™ ya 'Video DÃ¼zÉ™lt' bÃ¶lmÉ™sindÉ™n videomuzu hazÄ±rlayaq! ğŸ¬"
                    }
                }
            ]
        }
        
    try:
        # Ä°stifadÉ™Ã§inin son gÃ¶ndÉ™rdiyi sualÄ± gÃ¶tÃ¼rÃ¼rÃ¼k
        son_mesaj = istek.messages[-1].content
        
        # MÉ™tni rÉ™qÉ™mlÉ™rÉ™ (tokenlÉ™rÉ™) Ã§eviririk
        giriÅŸ_idlÉ™ri = torch.tensor([tokenizator.kodlasdir(son_mesaj)], dtype=torch.long, device=cihaz)
        
        # AI-dÉ™n yeni simvollar generasya etmÉ™sini istÉ™yirik
        with torch.no_grad():
            # generate() yerinÉ™ yeni_metn_yarat() istifadÉ™ edirik (model.py-da adÄ±nÄ± dÉ™yiÅŸmiÅŸik)
            butun_idlar = ismayil_modeli.yeni_metn_yarat(giriÅŸ_idlÉ™ri, maksimum_yeni_simvol=100)
            # YalnÄ±z AI-nin yaratdÄ±ÄŸÄ± hissÉ™ni kÉ™sib gÃ¶tÃ¼rÃ¼rÃ¼k (giriÅŸ mÉ™tnini Ã§Ä±xarÄ±q)
            cavab_idlari = butun_idlar[0][len(giriÅŸ_idlÉ™ri[0]):].tolist()
            # RÉ™qÉ™mlÉ™ri yenidÉ™n baÅŸa dÃ¼ÅŸÃ¼lÉ™n mÉ™tnÉ™ Ã§eviririk
            cavab_metni = tokenizator.de_kodlasdir(cavab_idlari)
            
        # OpenAI formatÄ±na uyÄŸun cavab qaytarÄ±rÄ±q (Frontend bunu gÃ¶zlÉ™yir)
        return {
            "choices": [
                {
                    "message": {
                        "role": "assistant",
                        "content": cavab_metni
                    }
                }
            ]
        }
    except Exception as xata:
        raise HTTPException(status_code=500, detail=str(xata))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¬ VÄ°DEO DÃœZÆLTMÆ ENDPOINT-LÆRÄ° (Kaggle + Stable Diffusion)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@ismayil_server.post("/video/edit")
async def video_duzelÑ‚(
    video: UploadFile = File(..., description="DÃ¼zÉ™ldilÉ™cÉ™k video fayl (mp4, avi, mov)"),
    prompt: str = Form(..., description="NecÉ™ gÃ¶rÃ¼nsÃ¼n? MÉ™s: 'oil painting style, colorful'")
):
    """
    Video faylÄ± vÉ™ prompt qÉ™bul edib Kaggle-a emal Ã¼Ã§Ã¼n gÃ¶ndÉ™rir.
    DÉ™rhal iÅŸ ID-si qaytarÄ±r â€” status-u /video/status/{job_id} ilÉ™ izlÉ™yin.
    """
    # Fayl nÃ¶vÃ¼nÃ¼ yoxlayÄ±rÄ±q
    icaze_verilmis = {"video/mp4", "video/avi", "video/quicktime", "video/x-msvideo"}
    if video.content_type and video.content_type not in icaze_verilmis:
        # Content-type hÉ™miÅŸÉ™ dÉ™qiq olmur, buna gÃ¶rÉ™ adÄ±nÄ± da yoxlayÄ±rÄ±q
        ad = (video.filename or "").lower()
        if not any(ad.endswith(x) for x in [".mp4", ".avi", ".mov", ".mkv"]):
            raise HTTPException(
                status_code=400,
                detail="YalnÄ±z video fayllarÄ± (mp4, avi, mov, mkv) qÉ™bul edilir."
            )
    
    # Video mÉ™zmununu oxuyuruq
    video_bytes = await video.read()
    
    if len(video_bytes) == 0:
        raise HTTPException(status_code=400, detail="BoÅŸ fayl gÃ¶ndÉ™rildi.")
    
    # Maksimum Ã¶lÃ§Ã¼ yoxlanÄ±ÅŸÄ± (500 MB)
    max_olchu = 500 * 1024 * 1024
    if len(video_bytes) > max_olchu:
        raise HTTPException(
            status_code=413,
            detail=f"Fayl hÉ™ddÉ™n bÃ¶yÃ¼kdÃ¼r. Maksimum: 500 MB, GÃ¶ndÉ™rilÉ™n: {len(video_bytes)//1024//1024} MB"
        )
    
    # Kaggle-a iÅŸ gÃ¶ndÉ™ririk
    is_id = is_gondÉ™r(
        video_bytes=video_bytes,
        prompt=prompt,
        fayl_adi=video.filename or "video.mp4"
    )
    
    return {
        "success": True,
        "job_id": is_id,
        "message": "Video emal Ã¼Ã§Ã¼n qÉ™bul edildi!",
        "kaggle_telimat": (
            f"ğŸ“‹ Kaggle Notebook-a gedin â†’ video_jobs/{is_id}/ qovluÄŸundakÄ± video + prompt.txt fayllarÄ±nÄ± "
            f"Kaggle Dataset-É™ yÃ¼klÉ™yin â†’ video_edit_worker.py notebook-unu iÅŸlÉ™din â†’ "
            f"nÉ™ticÉ™ /kaggle/working/output.mp4-da olacaq"
        ),
        "status_url": f"/video/status/{is_id}"
    }


@ismayil_server.get("/video/status/{is_id}")
async def video_status(is_id: str):
    """Ä°ÅŸ vÉ™ziyyÉ™tini yoxlayÄ±r: pending | processing | done | error"""
    return is_veziyyeti(is_id)


@ismayil_server.get("/video/download/{is_id}")
async def video_yukle(is_id: str):
    """TamamlanmÄ±ÅŸ Ã§Ä±xÄ±ÅŸ videosunu yÃ¼klÉ™mÉ™yÉ™ imkan verir."""
    melumat = is_veziyyeti(is_id)
    
    if melumat["status"] == "not_found":
        raise HTTPException(status_code=404, detail="Ä°ÅŸ tapÄ±lmadÄ±")
    
    if melumat["status"] != "done":
        raise HTTPException(
            status_code=202,
            detail=f"Video hÉ™lÉ™ hazÄ±r deyil. Cari vÉ™ziyyÉ™t: {melumat['status']}"
        )
    
    video_yolu = Path("video_jobs") / is_id / "output.mp4"
    if not video_yolu.exists():
        raise HTTPException(status_code=404, detail="Ã‡Ä±xÄ±ÅŸ faylÄ± tapÄ±lmadÄ±")
    
    return FileResponse(
        path=str(video_yolu),
        media_type="video/mp4",
        filename=f"ishayil_ai_video_{is_id}.mp4"
    )


@ismayil_server.get("/video/jobs")
async def butun_isler():
    """BÃ¼tÃ¼n video emal iÅŸlÉ™rinin siyahÄ±sÄ±."""
    return {"jobs": is_siyahisi()}


if __name__ == "__main__":
    # Railway vÉ™ digÉ™r serverlÉ™r PORT env variable istifadÉ™ edir
    # Lokal istifadÉ™ Ã¼Ã§Ã¼n default 8000
    port = int(os.environ.get("PORT", 8000))
    host = "0.0.0.0"  # BÃ¼tÃ¼n interfeysldÉ™n qulaq as
    print(f"ğŸš€ Ä°smayÄ±l AI server: http://{host}:{port}")
    print(f"   Kaggle API: {'ENV VARS' if os.environ.get('KAGGLE_KEY') else '~/.kaggle/kaggle.json'}")
    uvicorn.run(ismayil_server, host=host, port=port)
